{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (4001, 14)\n",
      "Test data shape: (999, 14)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['employment_status_Employed'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 699\u001b[0m\n\u001b[1;32m    696\u001b[0m     crm\u001b[38;5;241m.\u001b[39mvisualize_risk_distribution(expected_loss)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 699\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[22], line 677\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    673\u001b[0m ead_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan_term\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebt_to_income\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployment_status_Employed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    674\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployment_status_Self-employed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m crm\u001b[38;5;241m.\u001b[39mtrain_pd_model(train_data, pd_features, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_flag\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    678\u001b[0m crm\u001b[38;5;241m.\u001b[39mtrain_lgd_model(train_data, lgd_features, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgd\u001b[39m\u001b[38;5;124m'\u001b[39m, defaulted_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    679\u001b[0m crm\u001b[38;5;241m.\u001b[39mtrain_ead_model(train_data, ead_features, target_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mead_factor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 81\u001b[0m, in \u001b[0;36mCreditRiskModelingStatsmodels.train_pd_model\u001b[0;34m(self, train_data, features, target)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpd_features \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m X \u001b[38;5;241m=\u001b[39m train_data[features]\n\u001b[1;32m     82\u001b[0m X \u001b[38;5;241m=\u001b[39m add_constant(X)\n\u001b[1;32m     83\u001b[0m y \u001b[38;5;241m=\u001b[39m train_data[target]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['employment_status_Employed'] not in index\""
     ]
    }
   ],
   "source": [
    "class CreditRiskModelingStatsmodels:\n",
    "    def __init__(self):\n",
    "        self.pd_model = None\n",
    "        self.lgd_model = None\n",
    "        self.ead_model = None\n",
    "        self.pd_features = None\n",
    "        self.lgd_features = None\n",
    "        self.ead_features = None\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load credit data from CSV file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = pd.read_csv(filepath)\n",
    "            print(f\"Data loaded successfully with shape: {data.shape}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def preprocess_data(self, data, target_column='default_flag'):\n",
    "        \"\"\"\n",
    "        Preprocess data for modeling\n",
    "        \"\"\"\n",
    "        # Make a copy to avoid modifying the original\n",
    "        processed_data = data.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_cols = processed_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "        processed_data[numeric_cols] = processed_data[numeric_cols].fillna(processed_data[numeric_cols].median())\n",
    "        \n",
    "        categorical_cols = processed_data.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            processed_data[col] = processed_data[col].fillna(processed_data[col].mode()[0])\n",
    "        \n",
    "        # Create dummy variables for categorical columns\n",
    "        processed_data = pd.get_dummies(processed_data, columns=categorical_cols, drop_first=True)\n",
    "        \n",
    "        # Split data into train and test\n",
    "        np.random.seed(42)\n",
    "        mask = np.random.rand(len(processed_data)) < 0.8\n",
    "        train_data = processed_data[mask]\n",
    "        test_data = processed_data[~mask]\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Test data shape: {test_data.shape}\")\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def check_multicollinearity(self, X):\n",
    "        \"\"\"\n",
    "        Check for multicollinearity using Variance Inflation Factor (VIF)\n",
    "        \"\"\"\n",
    "        X_with_const = add_constant(X)\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = X_with_const.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
    "        \n",
    "        print(\"Variance Inflation Factors:\")\n",
    "        print(vif_data.sort_values('VIF', ascending=False))\n",
    "        \n",
    "        # Flag problematic features\n",
    "        high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Feature\"].tolist()\n",
    "        if \"const\" in high_vif_features:\n",
    "            high_vif_features.remove(\"const\")\n",
    "            \n",
    "        if high_vif_features:\n",
    "            print(f\"Warning: High multicollinearity detected in features: {high_vif_features}\")\n",
    "        \n",
    "        return vif_data\n",
    "    \n",
    "    def train_pd_model(self, train_data, features, target='default_flag'):\n",
    "        \"\"\"\n",
    "        Train Probability of Default model using Logistic Regression (Logit)\n",
    "        \"\"\"\n",
    "        # Store feature names for later prediction\n",
    "        self.pd_features = features\n",
    "        \n",
    "        # Prepare data\n",
    "        X = train_data[features]\n",
    "        X = add_constant(X)\n",
    "        y = train_data[target]\n",
    "        \n",
    "        # Check multicollinearity \n",
    "        self.check_multicollinearity(train_data[features])\n",
    "        \n",
    "        # Fit model\n",
    "        logit_model = Logit(y, X)\n",
    "        try:\n",
    "            self.pd_model = logit_model.fit(disp=0)  # disp=0 suppresses convergence messages\n",
    "            print(\"\\nPD Model Summary:\")\n",
    "            print(self.pd_model.summary())\n",
    "            \n",
    "            # Analyze p-values\n",
    "            params = self.pd_model.params\n",
    "            pvalues = self.pd_model.pvalues\n",
    "            print(\"\\nSignificant features (p < 0.05):\")\n",
    "            for feature, pvalue in zip(self.pd_model.model.exog_names, pvalues):\n",
    "                if pvalue < 0.05:\n",
    "                    print(f\"{feature}: coefficient = {params[feature]:.4f}, p-value = {pvalue:.4f}\")\n",
    "            \n",
    "            return self.pd_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting PD model: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def train_lgd_model(self, train_data, features, target='lgd', defaulted_only=True):\n",
    "        \"\"\"\n",
    "        Train Loss Given Default model using OLS regression\n",
    "        Default is to train only on defaulted loans\n",
    "        \"\"\"\n",
    "        # Store feature names for later prediction\n",
    "        self.lgd_features = features\n",
    "        \n",
    "        # Filter for defaulted loans if requested\n",
    "        if defaulted_only:\n",
    "            train_subset = train_data[train_data['default_flag'] == 1].copy()\n",
    "            if len(train_subset) < 10:\n",
    "                print(\"Warning: Not enough defaulted loans for LGD modeling\")\n",
    "                return None\n",
    "        else:\n",
    "            train_subset = train_data.copy()\n",
    "            \n",
    "        # Prepare data\n",
    "        X = train_subset[features]\n",
    "        X = add_constant(X)\n",
    "        y = train_subset[target]\n",
    "        \n",
    "        # Check multicollinearity\n",
    "        self.check_multicollinearity(train_subset[features])\n",
    "        \n",
    "        # Fit model using OLS\n",
    "        try:\n",
    "            self.lgd_model = sm.OLS(y, X).fit()\n",
    "            print(\"\\nLGD Model Summary:\")\n",
    "            print(self.lgd_model.summary())\n",
    "            \n",
    "            # Diagnostic plots\n",
    "            self.lgd_diagnostic_plots(self.lgd_model)\n",
    "            \n",
    "            # Calculate robust errors if there's heteroscedasticity\n",
    "            lgd_model_robust = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "            print(\"\\nLGD Model with Heteroscedasticity-Robust Standard Errors:\")\n",
    "            print(lgd_model_robust.summary())\n",
    "            \n",
    "            return self.lgd_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting LGD model: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def train_ead_model(self, train_data, features, target_factor='ead_factor'):\n",
    "        \"\"\"\n",
    "        Train Exposure at Default model using OLS regression\n",
    "        EAD is modeled as a factor of outstanding balance\n",
    "        \"\"\"\n",
    "        # Store feature names for later prediction\n",
    "        self.ead_features = features\n",
    "        \n",
    "        # Prepare data\n",
    "        X = train_data[features]\n",
    "        X = add_constant(X)\n",
    "        y = train_data[target_factor]\n",
    "        \n",
    "        # Check multicollinearity\n",
    "        self.check_multicollinearity(train_data[features])\n",
    "        \n",
    "        # Fit model\n",
    "        try:\n",
    "            self.ead_model = sm.OLS(y, X).fit()\n",
    "            print(\"\\nEAD Model Summary:\")\n",
    "            print(self.ead_model.summary())\n",
    "            \n",
    "            # Diagnostic plots\n",
    "            self.ead_diagnostic_plots(self.ead_model)\n",
    "            \n",
    "            return self.ead_model\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting EAD model: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def lgd_diagnostic_plots(self, model):\n",
    "        \"\"\"\n",
    "        Create diagnostic plots for LGD OLS model\n",
    "        \"\"\"\n",
    "        # Get model results\n",
    "        y = model.model.endog\n",
    "        y_pred = model.fittedvalues\n",
    "        resid = model.resid\n",
    "        \n",
    "        # Create figure with 4 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Residuals vs Fitted\n",
    "        axes[0, 0].scatter(y_pred, resid, edgecolors='k', facecolors='none')\n",
    "        axes[0, 0].axhline(y=0, color='k', linestyle='--')\n",
    "        axes[0, 0].set_xlabel('Fitted values')\n",
    "        axes[0, 0].set_ylabel('Residuals')\n",
    "        axes[0, 0].set_title('Residuals vs Fitted')\n",
    "        \n",
    "        # QQ plot\n",
    "        qq = ProbPlot(resid)\n",
    "        qq.qqplot(line='45', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Q-Q Plot')\n",
    "        \n",
    "        # Scale-Location\n",
    "        axes[1, 0].scatter(y_pred, np.sqrt(np.abs(resid)), edgecolors='k', facecolors='none')\n",
    "        axes[1, 0].set_xlabel('Fitted values')\n",
    "        axes[1, 0].set_ylabel('âˆš|Residuals|')\n",
    "        axes[1, 0].set_title('Scale-Location Plot')\n",
    "        \n",
    "        # Actual vs Predicted\n",
    "        axes[1, 1].scatter(y, y_pred, edgecolors='k', facecolors='none')\n",
    "        min_val = min(min(y), min(y_pred))\n",
    "        max_val = max(max(y), max(y_pred))\n",
    "        axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "        axes[1, 1].set_xlabel('Observed values')\n",
    "        axes[1, 1].set_ylabel('Predicted values')\n",
    "        axes[1, 1].set_title('Observed vs Predicted')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def ead_diagnostic_plots(self, model):\n",
    "        \"\"\" \n",
    "        Create diagnostic plots for EAD OLS model\n",
    "        \"\"\"\n",
    "        # Same as LGD diagnostic plots\n",
    "        self.lgd_diagnostic_plots(model)\n",
    "    \n",
    "    def evaluate_pd_model(self, test_data, target='default_flag'):\n",
    "        \"\"\"\n",
    "        Evaluate PD model\n",
    "        \"\"\"\n",
    "        if self.pd_model is None:\n",
    "            print(\"PD model not trained yet\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare data\n",
    "        X_test = test_data[self.pd_features]\n",
    "        X_test = add_constant(X_test)\n",
    "        y_test = test_data[target]\n",
    "        \n",
    "        # Predict probabilities\n",
    "        y_pred_proba = self.pd_model.predict(X_test)\n",
    "        \n",
    "        # Find optimal threshold using Youden's J statistic\n",
    "        thresholds = np.arange(0.1, 0.9, 0.02)\n",
    "        sensitivity = []\n",
    "        specificity = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "            # True positive rate (TPR) = sensitivity\n",
    "            tpr = sum((y_test == 1) & (y_pred == 1)) / sum(y_test == 1) if sum(y_test == 1) > 0 else 0\n",
    "            # True negative rate (TNR) = specificity\n",
    "            tnr = sum((y_test == 0) & (y_pred == 0)) / sum(y_test == 0) if sum(y_test == 0) > 0 else 0\n",
    "            \n",
    "            sensitivity.append(tpr)\n",
    "            specificity.append(tnr)\n",
    "        \n",
    "        # Calculate Youden's J statistic (J = sensitivity + specificity - 1)\n",
    "        j_scores = np.array(sensitivity) + np.array(specificity) - 1\n",
    "        best_idx = np.argmax(j_scores)\n",
    "        optimal_threshold = thresholds[best_idx]\n",
    "        \n",
    "        # Apply optimal threshold\n",
    "        y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        confusion = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        TP = confusion.loc[1, 1] if 1 in confusion.index and 1 in confusion.columns else 0\n",
    "        TN = confusion.loc[0, 0] if 0 in confusion.index and 0 in confusion.columns else 0\n",
    "        FP = confusion.loc[0, 1] if 0 in confusion.index and 1 in confusion.columns else 0\n",
    "        FN = confusion.loc[1, 0] if 1 in confusion.index and 0 in confusion.columns else 0\n",
    "        \n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Calculate AUC (Area Under ROC Curve)\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nPD Model Evaluation:\")\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Predicted vs Actual probability plot (calibration)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Bin the predictions\n",
    "        bins = 10\n",
    "        bin_edges = np.linspace(0, 1, bins + 1)\n",
    "        bin_indices = np.digitize(y_pred_proba, bin_edges[1:-1])\n",
    "        \n",
    "        bin_pred_probs = []\n",
    "        bin_actual_probs = []\n",
    "        bin_counts = []\n",
    "        \n",
    "        for i in range(bins):\n",
    "            mask = bin_indices == i\n",
    "            if sum(mask) > 0:  # Only consider bins with data\n",
    "                bin_pred_prob = np.mean(y_pred_proba[mask])\n",
    "                bin_actual_prob = np.mean(y_test[mask])\n",
    "                bin_count = sum(mask)\n",
    "                \n",
    "                bin_pred_probs.append(bin_pred_prob)\n",
    "                bin_actual_probs.append(bin_actual_prob)\n",
    "                bin_counts.append(bin_count)\n",
    "        \n",
    "        plt.scatter(bin_pred_probs, bin_actual_probs, s=[100 * c/sum(bin_counts) for c in bin_counts])\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('Predicted Probability')\n",
    "        plt.ylabel('Actual Probability')\n",
    "        plt.title('Calibration Plot (Reliability Diagram)')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'threshold': optimal_threshold,\n",
    "            'confusion_matrix': confusion,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': roc_auc\n",
    "        }\n",
    "    \n",
    "    def evaluate_lgd_model(self, test_data, target='lgd', defaulted_only=True):\n",
    "        \"\"\"\n",
    "        Evaluate LGD model\n",
    "        \"\"\"\n",
    "        if self.lgd_model is None:\n",
    "            print(\"LGD model not trained yet\")\n",
    "            return None\n",
    "            \n",
    "        # Filter for defaulted loans if requested\n",
    "        if defaulted_only:\n",
    "            test_subset = test_data[test_data['default_flag'] == 1].copy()\n",
    "            if len(test_subset) < 5:\n",
    "                print(\"Warning: Not enough defaulted loans in test data for LGD evaluation\")\n",
    "                return None\n",
    "        else:\n",
    "            test_subset = test_data.copy()\n",
    "        \n",
    "        # Prepare data\n",
    "        X_test = test_subset[self.lgd_features]\n",
    "        X_test = add_constant(X_test)\n",
    "        y_test = test_subset[target]\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = self.lgd_model.predict(X_test)\n",
    "        \n",
    "        # Clip predictions to valid range [0, 1]\n",
    "        y_pred = np.clip(y_pred, 0, 1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = np.mean((y_test - y_pred) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(y_test - y_pred))\n",
    "        r_squared = self.lgd_model.rsquared\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nLGD Model Evaluation:\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"R-squared: {r_squared:.4f}\")\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5, edgecolors='k')\n",
    "        plt.plot([0, 1], [0, 1], 'r--')\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.xlabel('Actual LGD')\n",
    "        plt.ylabel('Predicted LGD')\n",
    "        plt.title('LGD Model: Actual vs Predicted')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r_squared': r_squared\n",
    "        }\n",
    "    \n",
    "    def evaluate_ead_model(self, test_data, target_factor='ead_factor'):\n",
    "        \"\"\"\n",
    "        Evaluate EAD model\n",
    "        \"\"\"\n",
    "        if self.ead_model is None:\n",
    "            print(\"EAD model not trained yet\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare data\n",
    "        X_test = test_data[self.ead_features]\n",
    "        X_test = add_constant(X_test)\n",
    "        y_test = test_data[target_factor]\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = self.ead_model.predict(X_test)\n",
    "        \n",
    "        # Clip predictions to reasonable range (e.g., [0.5, 1.5])\n",
    "        y_pred = np.clip(y_pred, 0.5, 1.5)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = np.mean((y_test - y_pred) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(y_test - y_pred))\n",
    "        r_squared = self.ead_model.rsquared\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nEAD Model Evaluation:\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"R-squared: {r_squared:.4f}\")\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5, edgecolors='k')\n",
    "        min_val = min(min(y_test), min(y_pred))\n",
    "        max_val = max(max(y_test), max(y_pred))\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "        plt.xlabel('Actual EAD Factor')\n",
    "        plt.ylabel('Predicted EAD Factor')\n",
    "        plt.title('EAD Model: Actual vs Predicted')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r_squared': r_squared\n",
    "        }\n",
    "    \n",
    "    def predict_pd(self, data):\n",
    "        \"\"\"\n",
    "        Predict probability of default\n",
    "        \"\"\"\n",
    "        if self.pd_model is None:\n",
    "            print(\"PD model not trained yet\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features\n",
    "        X = data[self.pd_features]\n",
    "        X = add_constant(X)\n",
    "        \n",
    "        # Predict probabilities\n",
    "        pd_scores = self.pd_model.predict(X)\n",
    "        return pd_scores\n",
    "    \n",
    "    def predict_lgd(self, data):\n",
    "        \"\"\"\n",
    "        Predict loss given default\n",
    "        \"\"\"\n",
    "        if self.lgd_model is None:\n",
    "            print(\"LGD model not trained yet\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features\n",
    "        X = data[self.lgd_features]\n",
    "        X = add_constant(X)\n",
    "        \n",
    "        # Predict\n",
    "        lgd_scores = self.lgd_model.predict(X)\n",
    "        # Ensure LGD is between 0 and 1\n",
    "        lgd_scores = np.clip(lgd_scores, 0, 1)\n",
    "        return lgd_scores\n",
    "    \n",
    "    def predict_ead(self, data, outstanding_amounts):\n",
    "        \"\"\"\n",
    "        Predict exposure at default\n",
    "        \"\"\"\n",
    "        if self.ead_model is None:\n",
    "            print(\"EAD model not trained yet\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features\n",
    "        X = data[self.ead_features]\n",
    "        X = add_constant(X)\n",
    "        \n",
    "        # Predict EAD factor\n",
    "        ead_factors = self.ead_model.predict(X)\n",
    "        # Ensure EAD factor is reasonable\n",
    "        ead_factors = np.clip(ead_factors, 0.5, 1.5)\n",
    "        \n",
    "        # Calculate absolute EAD amounts\n",
    "        ead_amounts = ead_factors * outstanding_amounts\n",
    "        return ead_amounts\n",
    "    \n",
    "    def calculate_expected_loss(self, pd_scores, lgd_scores, ead_amounts):\n",
    "        \"\"\"\n",
    "        Calculate Expected Loss (EL) = PD * LGD * EAD\n",
    "        \"\"\"\n",
    "        expected_loss = pd_scores * lgd_scores * ead_amounts\n",
    "        return expected_loss\n",
    "    \n",
    "    def calculate_portfolio_metrics(self, expected_loss, confidence_levels=[0.95, 0.99]):\n",
    "        \"\"\"\n",
    "        Calculate portfolio-level risk metrics\n",
    "        \"\"\"\n",
    "        total_el = expected_loss.sum()\n",
    "        \n",
    "        metrics = {'Total Expected Loss': total_el}\n",
    "        \n",
    "        # Calculate VaR and CVaR at different confidence levels\n",
    "        for cl in confidence_levels:\n",
    "            var = np.percentile(expected_loss, cl * 100)\n",
    "            metrics[f'VaR ({cl*100}%)'] = var\n",
    "            \n",
    "            # Calculate CVaR (Expected Shortfall)\n",
    "            cvar = expected_loss[expected_loss >= var].mean()\n",
    "            metrics[f'CVaR ({cl*100}%)'] = cvar\n",
    "        \n",
    "        # Calculate distribution metrics\n",
    "        metrics['Mean EL'] = expected_loss.mean()\n",
    "        metrics['Median EL'] = np.median(expected_loss)\n",
    "        metrics['EL Standard Deviation'] = expected_loss.std()\n",
    "        metrics['EL Skewness'] = stats.skew(expected_loss)\n",
    "        metrics['EL Kurtosis'] = stats.kurtosis(expected_loss)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nPortfolio Risk Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: ${value:.2f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def visualize_risk_distribution(self, expected_loss, confidence_level=0.95):\n",
    "        \"\"\"\n",
    "        Visualize the distribution of expected loss\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Distribution plot\n",
    "        sns.histplot(expected_loss, kde=True, stat=\"density\", linewidth=0)\n",
    "        \n",
    "        # Add vertical lines for key metrics\n",
    "        mean_el = expected_loss.mean()\n",
    "        median_el = np.median(expected_loss)\n",
    "        var_el = np.percentile(expected_loss, confidence_level * 100)\n",
    "        cvar_el = expected_loss[expected_loss >= var_el].mean()\n",
    "        \n",
    "        plt.axvline(mean_el, color='r', linestyle='-', label=f'Mean: ${mean_el:.2f}')\n",
    "        plt.axvline(median_el, color='g', linestyle='--', label=f'Median: ${median_el:.2f}')\n",
    "        plt.axvline(var_el, color='b', linestyle='-.', \n",
    "                   label=f'{confidence_level*100}% VaR: ${var_el:.2f}')\n",
    "        plt.axvline(cvar_el, color='m', linestyle=':', \n",
    "                   label=f'{confidence_level*100}% CVaR: ${cvar_el:.2f}')\n",
    "        \n",
    "        plt.xlabel('Expected Loss ($)')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Distribution of Expected Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # QQ Plot for expected loss\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        stats.probplot(expected_loss, dist=\"norm\", plot=plt)\n",
    "        plt.title('QQ Plot of Expected Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def generate_sample_data(n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate sample credit risk data for demonstration\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Borrower characteristics\n",
    "    age = np.random.normal(40, 10, n_samples)\n",
    "    income = np.random.lognormal(10, 0.5, n_samples) * 1000\n",
    "    debt_to_income = np.random.beta(2, 5, n_samples)\n",
    "    credit_score = np.random.normal(700, 100, n_samples)\n",
    "    loan_amount = np.random.lognormal(10, 0.7, n_samples) * 1000\n",
    "    loan_term = np.random.choice([12, 24, 36, 48, 60], n_samples)\n",
    "    \n",
    "    # Employment status\n",
    "    employment_status = np.random.choice(['Employed', 'Self-employed', 'Unemployed', 'Retired'], \n",
    "                                          n_samples, p=[0.7, 0.2, 0.05, 0.05])\n",
    "    \n",
    "    # Create probability of default (influenced by features)\n",
    "    pd_score = (0.3 * stats.norm.cdf(-credit_score, loc=600, scale=150) + \n",
    "                0.3 * debt_to_income + \n",
    "                0.2 * (np.log(loan_amount) - np.log(income)) + \n",
    "                0.2 * np.random.normal(0, 0.1, n_samples))\n",
    "    pd_score = np.clip(pd_score, 0.001, 0.999)\n",
    "    \n",
    "    # Default flag based on probability\n",
    "    default_flag = np.random.binomial(1, pd_score)\n",
    "    \n",
    "    # LGD only relevant for defaulted loans\n",
    "    lgd = np.zeros(n_samples)\n",
    "    defaulted = default_flag == 1\n",
    "    lgd[defaulted] = 0.4 + 0.3 * np.random.beta(2, 2, sum(defaulted)) + 0.1 * debt_to_income[defaulted]\n",
    "    lgd = np.clip(lgd, 0, 1)\n",
    "    \n",
    "    # EAD as a percentage of loan amount\n",
    "    ead_factor = 0.8 + 0.2 * np.random.beta(5, 2, n_samples)\n",
    "    outstanding_balance = loan_amount * np.random.uniform(0.3, 1.0, n_samples)  # Simulate partially paid loans\n",
    "    ead_amount = ead_factor * outstanding_balance\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'income': income,\n",
    "        'debt_to_income': debt_to_income,\n",
    "        'credit_score': credit_score,\n",
    "        'loan_amount': loan_amount,\n",
    "        'loan_term': loan_term,\n",
    "        'employment_status': employment_status,\n",
    "        'outstanding_balance': outstanding_balance,\n",
    "        'default_flag': default_flag,\n",
    "        'lgd': lgd,\n",
    "        'ead_factor': ead_factor,\n",
    "        'ead_amount': ead_amount\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate credit risk modeling using statsmodels\n",
    "    \"\"\"\n",
    "    # Either load your own data or generate sample data\n",
    "    # data = load_data('your_credit_data.csv')\n",
    "    data = generate_sample_data(n_samples=5000)\n",
    "    \n",
    "    # Initialize the credit risk model\n",
    "    crm = CreditRiskModelingStatsmodels()\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_data, test_data = crm.preprocess_data(data, target_column='default_flag')\n",
    "    \n",
    "    # Define features for each model\n",
    "    pd_features = ['age', 'income', 'debt_to_income', 'credit_score', \n",
    "                  'loan_amount', 'loan_term', 'employment_status_Employed', \n",
    "                  'employment_status_Retired', 'employment_status_Self-employed']\n",
    "    \n",
    "    lgd_features = ['debt_to_income', 'loan_amount', 'loan_term', \n",
    "                   'credit_score', 'employment_status_Employed']\n",
    "    \n",
    "    ead_features = ['loan_term', 'debt_to_income', 'employment_status_Employed',\n",
    "                   'employment_status_Self-employed']\n",
    "    \n",
    "    # Train models\n",
    "    crm.train_pd_model(train_data, pd_features, target='default_flag')\n",
    "    crm.train_lgd_model(train_data, lgd_features, target='lgd', defaulted_only=True)\n",
    "    crm.train_ead_model(train_data, ead_features, target_factor='ead_factor')\n",
    "    \n",
    "    # Evaluate models\n",
    "    crm.evaluate_pd_model(test_data, target='default_flag')\n",
    "    crm.evaluate_lgd_model(test_data, target='lgd', defaulted_only=True)\n",
    "    crm.evaluate_ead_model(test_data, target_factor='ead_factor')\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    pd_scores = crm.predict_pd(test_data)\n",
    "    lgd_scores = crm.predict_lgd(test_data)\n",
    "    ead_amounts = crm.predict_ead(test_data, test_data['outstanding_balance'])\n",
    "    \n",
    "    # Calculate expected loss\n",
    "    expected_loss = crm.calculate_expected_loss(pd_scores, lgd_scores, ead_amounts)\n",
    "    \n",
    "    # Analyze portfolio risk\n",
    "    portfolio_metrics = crm.calculate_portfolio_metrics(expected_loss)\n",
    "    crm.visualize_risk_distribution(expected_loss)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
